# 交叉熵和散度

​	

## 交叉熵

对于分布为p(x)的随机变量，熵H(p)表示其最优编码长度。交叉熵（Cross Entropy）是按照概率分布q的最优编码对真实分布为p的信息进行编码的长度，定义为：

$$
\begin{aligned}
H(p, q) &=\mathbb{E}_{p}[-\log q(x)] \\
&=-\sum_{x} p(x) \log q(x)
\end{aligned}
$$

在给定p的情况下，如果q和p越接近，交叉熵越小；如果q和p越远，交叉熵就越大。

## KL散度

​	*KL* 散度（Kullback-Leibler Divergence），也叫*KL* 距离或相对熵(Relative Entropy)，是用概率分布q来近似p时所造成的信息损失量。KL散度是按照概率分布q的最优编码对真实分布为p的信息进行编码，其平均编码长度（即交叉熵）H(p, q)和p的最优平均编码长度（即熵）H(p)之间的差异。 对于离散概率分布p 和q，从q到p的KL散度定义为：

$$
\begin{aligned}
\mathrm{KL}(p, q) &=H(p, q)-H(p) \\
&=\sum_{x} p(x) \log \frac{p(x)}{q(x)}
\end{aligned}
$$

其中为了保证连续性，定义
$$ 0 \log \frac{0}{0}=0,0 \log \frac{0}{q}=0$$

KL 散度总是非负的，$KL(𝑝, 𝑞) ≥ 0$，可以衡量两个概率分布之间的距离。KL散度只有当p = q时，$KL(𝑝, 𝑞) = 0$。如果两个分布越接近，KL散度越小；如果两个分布越远，KL散度就越大。 但KL散度**并不是**一个**真正的度量或距离**。

- KL散度不满足距离的对称性
- KL散度不满足距离的三角不等式性质

### 正向KL散度$KL(p||q)$

$$
\hat{q}=\operatorname{argmin}_{q} \int_{x} p(x) \log \frac{p(x)}{q(x)} d x
$$

仔细观察(3)式，$p(x)$是已知的真实分布，要求使上式最小的$q(x)$。

考虑当 $p(x)=0$ 时，这时$q(x)$取任何值都可以，因为$log \frac{p(x)}{q(x)}$这一项对整体的KL散度没有影响。当$p(x)>0$时，这一项对$log \frac{p(x)}{q(x)}$整体的KL散度就会产生影响，为了使(3)式最小，$q(x)$又处于$log \frac{p(x)}{q(x)}$中分母的位置，所以尽量大一些才好$q(x)$。

总体而言，对于正向 KL 散度，在$p(x)$大的地方，想让 KL 散度小，就需要$q(x)$的值也尽量大；在$p(x)$小的地方，$q(x)$对整体 KL 影响并不大（因为 log 项本身分子很小，又乘了一个非常小的$p(x)$）。换一种说法，要想使正向 KL 散度最小，则要求在 $p$ 不为 0 的地方，$q$ 也尽量不为 0，所以正向 KL 散度被称为是 zero avoiding。此时得到的分布 $q$ 是一个比较 “宽” 的分布。

### 反向KL散度$KL(q||p)$

$$
\hat{q}=\operatorname{argmin}_{q} \int_{x} q(x) \log \frac{q(x)}{p(x)} d x
$$

仔细观察(4)式，$p(x)$是已知的真实分布，要求使上式最小的$q(x)$。

考虑当 $p(x)=0$ 时，这时为了使(4)式变小，$q(x)$取0值才可以，否则(4)式就会变成无穷大。当$p(x)>0$时，为了使(4)式变小，必须在$p(x)$小的地方，$q(x)$也小。在$p(x)$大的地方可以适当忽略。换一种说法，要想使反向 KL 散度最小，则要求在 $p$ 为 0 的地方，$q$ 也尽量为 0，所以反向 KL 散度被称为是 zero forcing。此时得到分布 $q$ 是一个比较 “窄” 的分布。

#### 一个例子

假如$p(x)$是两个高斯分布的混合，$q(x)$是单个高斯，用$q(x)$去近似$p(x)$，两种KL散度该如何选择？

![image-20211022203316584](https://gitee.com/shenhao-stu/picgo/raw/master/%20DataWhale/image-20211022203316584.png)

对于正向KL散度来说，$q(x)$的分布图像更符合第二行，正向KL散度更在意中$p(x)$的常见事件，也就是首先要保证$p(x)$峰值附近的$x$，在$q(x)$中的概率密度值不能为0。当 $p$ 具有多个峰时，$q$ 选择将这些峰模糊到一起，以便将高概率质量放到所有峰上。

对于反向KL散度来说，$q(x)$的分布图像更符合第一行。反向KL散度更在意中$p(x)$的罕见事件，也就是首先要保证$p(x)$低谷附件的$x$，在$q(x)$中的概率密度值也较小。当 $p$ 具有多个峰并且这些峰间隔很宽时，如该图所示，最小化 KL 散度会选择单个峰，以避免将概率密度放置在 $p$ 的多个峰之间的低概率区域中。

## JS散度

*JS* 散度（Jensen-Shannon Divergence）是一种对称的衡量两个分布相似度

的度量方式，定义为：

$$
\mathrm{JS}(p, q)=\frac{1}{2} \mathrm{KL}(p, m)+\frac{1}{2} \mathrm{KL}(q, m)
$$

其中$m=\frac{1}{2} (p+q) $。

JS 散度是 KL 散度一种改进。但两种散度都存在一个问题，即如果两个分布p, q没有重叠或者重叠非常少时，KL散度和JS散度都很难衡量两个分布的距离。

## Wasserstein距离

*Wasserstein*距离（Wasserstein Distance）也用于衡量两个分布之间的距离。对于两个分布$q_1$ , $q_2$，$p^{th}-Wasserstein$距离定义为：
$$
W_{p}\left(q_{1}, q_{2}\right)=\left(\inf _{\gamma(x, y) \in \Gamma\left(q_{1}, q_{2}\right)} \mathbb{E}_{(x, y) \sim \gamma(x, y)}\left[d(x, y)^{p}\right]\right)^{\frac{1}{p}}
$$
其中$\Gamma\left(q_{1}, q_{2}\right)$是边际分布为$q_1$ , $q_2$ 的所有可能的联合分布集合,$𝑑(𝑥, 𝑦)$为x和y的距离，比如$$\ell_{p}$$距离等等

如果将两个分布看作是两个土堆，联合分布 $\gamma(x, y)$ 看作是从土堆 q1 的位置x到土堆q2 的位置y的搬运土的数量，并有

$$
\begin{array}{l}
\sum_{x} \gamma(x, y)=q_{2}(y) \\
\sum_{y} \gamma(x, y)=q_{1}(x)
\end{array}
$$

$\mathbb{E}_{(x, y) \sim \gamma(x, y)}\left[d(x, y)^{p}\right]$

可以理解为在联合分布$\gamma(x, y)$下把形状为$q_1$的土堆搬运到形状为$q_2$ 的土堆所需的工作量：

$$
\mathbb{E}_{(x, y) \sim \gamma(x, y)}\left[d(x, y)^{p}\right]=\sum_{(x, y)} \gamma(x, y) d(x, y)^{p}
$$

其中从土堆$q_1$中的点 x 到土堆 $q_2$中的点 y 的移动土的数量和距离分别为$𝛾(𝑥, 𝑦)$和$d(x, y)^{p}$ 。因此，Wasserstein 距离可以理解为搬运土堆的最小工作量，也称为推土机距离（Earth-Mover’s Distance，EMD）。 

Wasserstein 距离相比 KL 散度和 JS 散度的优势在于：即使两个分布没有重叠或者重叠非常少，Wasserstein距离仍然能反映两个分布的远近。

对于 $\mathbb{R}^{N}$ 空间中的两个高斯分布 $p=\mathcal{N}\left(\mu_{1}, \Sigma_{1}\right)$ 和 $q=\mathcal{N}\left(\mu_{2}, \Sigma_{2}\right)$, 它们的$2^{\text {nd }}$ Wasserstein 距离为

$$
W_{2}(p, q)=\left\|\mu_{1}-\mu_{2}\right\|_{2}^{2}+\operatorname{tr}\left(\Sigma_{1}+\Sigma_{2}-2\left(\Sigma_{2}^{\frac{1}{2}} \Sigma_{1} \Sigma_{2}^{\frac{1}{2}}\right)^{\frac{1}{2}}\right)
$$

当两个分布的方差为 0 时，$2^{nd}-Wasserstein$ 距离等价于欧氏距离。

