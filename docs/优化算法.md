网络优化



# 深度学习中的优化算法

在这里建立了基于梯度的优化算法的基本分析框架，并讨论了它如何应用在深度学习中。

1. Gradient descent 

2. 1. Formalizing the Taylor Expansion 
   2. Descent lemma for gradient descent

3. Stochastic gradient descent 

4. Accelerated Gradient Descent 

5. Local Runtime Analysis of GD

6. 1. Pre-conditioners 

------

### 梯度下降（GD）

假设我们现在想要找出一个多元连续函数 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Comega%29) 的最小值 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmin+_%7Bw+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D%7D+f%28w%29)

梯度下降算法是这样的： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+w_%7B0%7D+%26%3D%5Ctext+%7B+initializaiton+%7D+%5C%5C+w_%7Bt%2B1%7D+%26%3Dw_%7Bt%7D-%5Ceta+%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29+%5Cend%7Baligned%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 称为步长或学习率。

设计出GD算法的一个核心思想就是找出局部最陡的梯度下降方向 ![[公式]](https://www.zhihu.com/equation?tex=-%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29)

我们来考虑该点的泰勒展开式： ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29%3Df%5Cleft%28w_%7Bt%7D%5Cright%29%2B%5Cunderbrace%7B%5Cleft%5Clangle%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29%2C+w-w_%7Bt%7D%5Cright%5Crangle%7D_%7B%5Ctext+%7Blinear+in+%7D+w%7D%2B%5Ccdots)

假设我们去掉高阶项，只在 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bt%7D) 的一个邻域内优化一阶近似式，即 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bc%7D%5Cunderset%7Bw+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D%7D%7B%5Carg+%5Cmin+%7D+f%5Cleft%28w_%7Bt%7D%5Cright%29%2B%5Cleft%5Clangle%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29%2C+w-w_%7Bt%7D%5Cright%5Crangle+%5C%5C+%5Ctext+%7B+s.t.+%7D%5Cleft%5C%7Cw-w_%7Bt%7D%5Cright%5C%7C_%7B2%7D+%5Cleq+%5Cepsilon%5Cend%7Barray%7D) ，它的最优解是 ![[公式]](https://www.zhihu.com/equation?tex=w%2B%5Cdelta) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%3D-%5Calpha+%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29)

**泰勒展开的形式化**

我们下面来陈述一个引理，它刻画了GD算法下函数值的下降。我们先假设函数 ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29) 的二阶梯度是有界的，即 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla%5E%7B2%7D+f%28w%29+%5Cin+%5B-L%2CL%5D+++%5C%2C%5C%2C%5C%2C%5C%2C%5Cforall+w) ，我们称满足这个条件的函数为L-光滑函数。

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%5Ctext+%7B+Definition%7D%5Cleft%28L+%5Ctext+%7B+-smoothness%29.+%7D+%5Ctext+%7B+A+function+%7D+f%3A+%5Cmathbb%7BR%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BR%7D+%5Ctext+%7B+is+called+%7D+L+%5Ctext+%7B+-smooth+iffor+all+%7D+x%2C+y+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%7D%5Cright.+%5Ctext+%7B+%2C+the+following+inequality+%7D+%5C%5C+%5Ctext+%7B+holds%3A+%7D+%5C%5C+%5Cqquad%5C%7C%5Cnabla+f%28x%29-%5Cnabla+f%28y%29%5C%7C+%5Cleq+L%5C%7Cx-y%5C%7C+.+%5C%5C+%5Ctext+%7B+If+the+function+%7D+f+%5Ctext+%7B+is+%7D+L+%5Ctext+%7B+-smooth%2C+then+for+all+%7D+x%2C+y+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%7D+%5C%5C+%5Cqquad+f%28y%29+%5Cleq+f%28x%29%2B%5Clangle%5Cnabla+f%28x%29%2C+y-x%5Crangle%2B%5Cfrac%7BL%7D%7B2%7D%5C%7Cy-x%5C%7C%5E%7B2%7D+.+%5C%5C+%5Ctext+%7B+Next%2C+if+%7D+f+%5Ctext+%7B+is+additionally+convex+and+%7D+x%5E%7B%2A%7D+%5Ctext+%7B+is+its+minimizer%2C+then+for+all+%7D+x+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D+%5C%5C+%5Cqquad%5C%7C%5Cnabla+f%28x%29%5C%7C%5E%7B2%7D+%5Cleq+2+L%5Cleft%28f%28x%29-f%5Cleft%28x%5E%7B%2A%7D%5Cright%29%5Cright%29%5Cend%7Barray%7D)

这使我们能够在下面这个意义上使用泰勒展开精确的近似函数： ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29+%5Cleq+f%5Cleft%28w_%7Bt%7D%5Cright%29%2B%5Cleft%5Clangle%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29%2C+w-w_%7Bt%7D%5Cright%5Crangle%2B%5Cfrac%7BL%7D%7B2%7D%5Cleft%5C%7Cw-w_%7Bt%7D%5Cright%5C%7C_%7B2%7D%5E%7B2%7D)

**GD的下降引理**

下面我们将说明，在下降梯度和足够小的学习率下，函数值总是减小，除非迭代处的梯度为零。

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%5Ctext+%7B+Lemma+%28Descent+Lemma%29.+Suppose+%7D+f+%5Ctext+%7B+is+%7D+L+%5Ctext+%7B+-smooth.+Then%2C+if+%7D+%5Ceta%3C+%5C%5C+1+%2F%282+L%29%2C+%5Ctext+%7B+we+have+%7D+%5C%5C+%5Cqquad+f%5Cleft%28w_%7Bt%2B1%7D%5Cright%29+%5Cleq+f%5Cleft%28w_%7Bt%7D%5Cright%29-%5Cfrac%7B%5Ceta%7D%7B2%7D+%5Ccdot%5Cleft%5C%7C%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29%5Cright%5C%7C_%7B2%7D%5E%7B2%7D%5Cend%7Barray%7D)

证明：

![img](https://pic4.zhimg.com/80/v2-c42273cfcad6ef828ae9e50f5ddfb953_1440w.jpg)

### 随机梯度下降（SGD）

Motivation：损失函数的梯度的计算代价可能很大。

在深度学习里，目标函数通常是训练数据集中有关各个样本的损失函数的平均。设 ![[公式]](https://www.zhihu.com/equation?tex=f_i%28%5Cboldsymbol%7Bx%7D%29) 是有关索引为i的训练数据样本的损失函数，n是训练数据样本数， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D) 是模型的参数向量，那么目标函数定义为

![[公式]](https://www.zhihu.com/equation?tex=f%28%5Cboldsymbol%7Bx%7D%29+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi+%3D+1%7D%5En+f_i%28%5Cboldsymbol%7Bx%7D%29.)

目标函数在 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D) 处的梯度计算为

![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Cboldsymbol%7Bx%7D%29+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi+%3D+1%7D%5En+%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29.)

如果使用梯度下降，每次自变量迭代的计算开销为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%29) ，它随着n线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。

随机梯度下降（stochastic gradient descent，SGD）减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引 ![[公式]](https://www.zhihu.com/equation?tex=i%5Cin%5C%7B1%2C%5Cldots%2Cn%5C%7D) ，并计算梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29) 来迭代 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D) ：

![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D+%5Cleftarrow+%5Cboldsymbol%7Bx%7D+-+%5Ceta+%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29.)

这里 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 同样是学习率。可以看到，每次迭代的计算开销从梯度下降的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%29) 降到了常数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%281%29)。值得强调的是，随机梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29) 是对梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Cboldsymbol%7Bx%7D%29) 的无偏估计：

![[公式]](https://www.zhihu.com/equation?tex=E_i+%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi+%3D+1%7D%5En+%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29+%3D+%5Cnabla+f%28%5Cboldsymbol%7Bx%7D%29.)

这意味着，平均来说，随机梯度是对梯度的一个良好的估计。

### 加速梯度下降（AGD）

让我们考虑一个输入和输出分别为二维向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D+%3D+%5Bx_1%2C+x_2%5D%5E%5Ctop) 和标量的目标函数 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Cboldsymbol%7Bx%7D%29%3D0.1x_1%5E2%2B2x_2%5E2) 。

![img](https://pic1.zhimg.com/80/v2-3c1f2b7c5442fb2a52b4a92656529368_1440w.jpg)

可以看到，同一位置上，目标函数在竖直方向（ ![[公式]](https://www.zhihu.com/equation?tex=x_2) 轴方向）比在水平方向（ ![[公式]](https://www.zhihu.com/equation?tex=x_1) 轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。

学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。

![img](https://pic4.zhimg.com/80/v2-fc41ebaa6f404343a2c1f3090afbc67b_1440w.jpg)

动量法的提出是为了解决梯度下降的上述问题。设时间步t的自变量为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D_t) ，学习率为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_t) 。 在时间步0，动量法创建速度变量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bv%7D_0) ，并将其元素初始化成0。在时间步t>0，动量法对每次迭代的步骤做如下修改：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D%5Cbegin%7Baligned%7D+%5Cboldsymbol%7Bv%7D_t+%26%5Cleftarrow+%5Cgamma+%5Cboldsymbol%7Bv%7D_%7Bt-1%7D+%2B+%5Ceta_t+%5Cboldsymbol%7Bg%7D_t%2C+%5C%5C+%5Cboldsymbol%7Bx%7D_t+%26%5Cleftarrow+%5Cboldsymbol%7Bx%7D_%7Bt-1%7D+-+%5Cboldsymbol%7Bv%7D_t%2C+%5Cend%7Baligned%7D%5Cend%7Bsplit%7D)

其中，动量超参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 满足 ![[公式]](https://www.zhihu.com/equation?tex=0+%5Cleq+%5Cgamma+%3C+1) 。当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%3D0) 时，动量法等价于小批量随机梯度下降。

指数加权移动平均：为了从数学上理解动量法，让我们先解释一下指数加权移动平均（exponentially weighted moving average）。给定超参数 ![[公式]](https://www.zhihu.com/equation?tex=0+%5Cleq+%5Cgamma+%3C+1) ，当前时间步t的变量 ![[公式]](https://www.zhihu.com/equation?tex=y_t)是上一时间步t-1的变量 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt-1%7D) 和当前时间步另一变量 ![[公式]](https://www.zhihu.com/equation?tex=x_t) 的线性组合：

![[公式]](https://www.zhihu.com/equation?tex=y_t+%3D+%5Cgamma+y_%7Bt-1%7D+%2B+%281-%5Cgamma%29+x_t.)

我们可以对 ![[公式]](https://www.zhihu.com/equation?tex=y_t) 展开：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D%5Cbegin%7Baligned%7D+y_t+%26%3D+%281-%5Cgamma%29+x_t+%2B+%5Cgamma+y_%7Bt-1%7D%5C%5C+%26%3D+%281-%5Cgamma%29x_t+%2B+%281-%5Cgamma%29+%5Ccdot+%5Cgamma+x_%7Bt-1%7D+%2B+%5Cgamma%5E2y_%7Bt-2%7D%5C%5C+%26%3D+%281-%5Cgamma%29x_t+%2B+%281-%5Cgamma%29+%5Ccdot+%5Cgamma+x_%7Bt-1%7D+%2B+%281-%5Cgamma%29+%5Ccdot+%5Cgamma%5E2x_%7Bt-2%7D+%2B+%5Cgamma%5E3y_%7Bt-3%7D%5C%5C+%26%5Cldots+%5Cend%7Baligned%7D%5Cend%7Bsplit%7D)

令 ![[公式]](https://www.zhihu.com/equation?tex=n+%3D+1%2F%281-%5Cgamma%29) ，那么 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%281-1%2Fn%5Cright%29%5En+%3D+%5Cgamma%5E%7B1%2F%281-%5Cgamma%29%7D) 。因为

![[公式]](https://www.zhihu.com/equation?tex=%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D+%5Cleft%281-%5Cfrac%7B1%7D%7Bn%7D%5Cright%29%5En+%3D+%5Cexp%28-1%29+%5Capprox+0.3679%2C)

所以当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+%5Crightarrow+1) 时， ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B1%2F%281-%5Cgamma%29%7D%3D%5Cexp%28-1%29) ，如 ![[公式]](https://www.zhihu.com/equation?tex=0.95%5E%7B20%7D+%5Capprox+%5Cexp%28-1%29) 。如果把 ![[公式]](https://www.zhihu.com/equation?tex=%5Cexp%28-1%29) 当作一个比较小的数，我们可以在近似中忽略所有含 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B1%2F%281-%5Cgamma%29%7D) 和比 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B1%2F%281-%5Cgamma%29%7D) 更高阶的系数的项。例如，当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%3D0.95) 时，

![[公式]](https://www.zhihu.com/equation?tex=y_t+%5Capprox+0.05+%5Csum_%7Bi%3D0%7D%5E%7B19%7D+0.95%5Ei+x_%7Bt-i%7D.)

因此，在实际中，我们常常将 ![[公式]](https://www.zhihu.com/equation?tex=y_t) 看作是对最近 ![[公式]](https://www.zhihu.com/equation?tex=1%2F%281-%5Cgamma%29) 个时间步的 ![[公式]](https://www.zhihu.com/equation?tex=x_t) 值的加权平均。例如，当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+%3D+0.95) 时， ![[公式]](https://www.zhihu.com/equation?tex=y_t) 可以被看作对最近20个时间步的 ![[公式]](https://www.zhihu.com/equation?tex=x_t) 值的加权平均；当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+%3D+0.9) 时， ![[公式]](https://www.zhihu.com/equation?tex=y_t) 可以看作是对最近10个时间步的 ![[公式]](https://www.zhihu.com/equation?tex=x_t) 值的加权平均。而且，离当前时间步t越近的 ![[公式]](https://www.zhihu.com/equation?tex=x_t) 值获得的权重越大（越接近1）。

现在，我们对动量法的速度变量做变形：
![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bv%7D_t+%5Cleftarrow+%5Cgamma+%5Cboldsymbol%7Bv%7D_%7Bt-1%7D+%2B+%281+-+%5Cgamma%29+%5Cleft%28%5Cfrac%7B%5Ceta_t%7D%7B1+-+%5Cgamma%7D+%5Cboldsymbol%7Bg%7D_t%5Cright%29.) 
由指数加权移动平均的形式可得，速度变量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bv%7D_t) 实际上对序列 ![[公式]](https://www.zhihu.com/equation?tex=%5C%7B%5Ceta_%7Bt-i%7D%5Cboldsymbol%7Bg%7D_%7Bt-i%7D+%2F%281-%5Cgamma%29%3Ai%3D0%2C%5Cldots%2C1%2F%281-%5Cgamma%29-1%5C%7D) 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 ![[公式]](https://www.zhihu.com/equation?tex=1%2F%281-%5Cgamma%29) 个时间步的更新量做了指数加权移动平均后再除以 ![[公式]](https://www.zhihu.com/equation?tex=1-%5Cgamma) 。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个方向上是否一致。

### 本地运行时间分析

当迭代接近局部极小值时，梯度下降行为更为明显，因为该函数可以用二次函数进行局部逼近。因此这里为了简单起见，我们假设我们正在优化一个凸二次函数，并了解函数的曲率如何影响算法的收敛性。

我们用梯度下降方法来优化 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmin+_%7Bw%7D+%5Cfrac%7B1%7D%7B2%7D+w%5E%7B%5Ctop%7D+A+w) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=w+%5Cin+R%5Ed) ，![[公式]](https://www.zhihu.com/equation?tex=A%5Cin+R%5E%7Bd+%5Ctimes+d%7D) 是半正定矩阵。
注：w.l.o.g，我们可以假设A是对角矩阵（对角化是线性代数中的一个基本idea）。假设A的SVD分解： ![[公式]](https://www.zhihu.com/equation?tex=A+%3D+U%5CSigma+U%5E%7BT%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5CSigma) 是个对角矩阵。我们可以简单的验证得到 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7BT%7DAw+%3D+%5Chat%7Bw%7D%5E%7BT%7D%5CSigma+%5Chat%7Bw%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bw%7D+%3D+U%5E%7BT%7Dw) 。换句话说，在由U定义的一个不同的坐标系中，我们处理的是一个以对角矩阵 ![[公式]](https://www.zhihu.com/equation?tex=%5CSigma) 为系数的二次型。注意这里的对角化技术仅用于分析。

因此我们假设 ![[公式]](https://www.zhihu.com/equation?tex=A+%3D+diag%28%5Clambda_%7B1%7D%2C%5Ccdots%2C%5Clambda_d%29) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_1%5Cgeq+%5Clambda_2%5Cgeq+%5Ccdots+%5Cgeq+%5Clambda_d) ，这样该函数就可以化简为 ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Csum%5E%7Bd%7D_%7Bi+%3D+1%7D%5Clambda_i+w_i%5E%7B2%7D) ，这样梯度下降更新可以写成： ![[公式]](https://www.zhihu.com/equation?tex=x+%5Cleftarrow+w+-+%5Ceta+%5Cnabla+f%28w%29+%3D+w-%5Ceta%5CSigma+w)

### **Pre-conditioners**

从上面的二次型例子中，我们可以看到如果我们在不同的坐标系中使用不同的学习率，这将是得到优化。换句话说，如果我们对每个坐标引入一个学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_i+%3D+1%2F%5Clambda_i) ，那么我们可以实现更快的收敛。

在A不是对角阵这样的更一般的情况下，我们事先不知道坐标系，算法对应于 ![[公式]](https://www.zhihu.com/equation?tex=w%5Cleftarrow+w-A%5E%7B-1%7D+%5Cnabla+f%28w%29)

在更一般的情况下，f不是二次函数，这与牛顿算法相对应 ![[公式]](https://www.zhihu.com/equation?tex=w+%5Cleftarrow+w-%5Cnabla%5E2+f%28w%29%5E%7B-1%7D%5Cnabla+f%28w%29)

计算Hessian矩阵可能是非常困难的，因为它scale quadratically in d（在实践中可能超过100万）。因此，使用hessian函数及其逆函数的近似值。